{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18d81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Suppressing tf.hub warnings\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "622bcc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "conn = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853172e",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a94562",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv('./text/text-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b268376b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50282, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e16924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(file):\n",
    "    if file in os.listdir('./images/train'):\n",
    "        return 'train'\n",
    "    elif file in os.listdir('./images/valid'):\n",
    "        return 'valid'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e206bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['dataset'] = annotations['image'].apply(lambda x: check_folder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e9085c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[annotations.dataset == 'valid'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a79aa24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[annotations.dataset == 'train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "067ec7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_annotations = annotations[(annotations.dataset == 'valid') | (annotations.dataset == 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "435393d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d211f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/ec2-user/SageMaker/'\n",
    "images_dir = os.path.join(root_dir, 'images')\n",
    "tfrecords_dir = os.path.join(root_dir, \"tfrecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2de41b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 10000\n"
     ]
    }
   ],
   "source": [
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for i in range(len(req_annotations)):\n",
    "    text = list(req_annotations.text)[i]\n",
    "    dataset = list(req_annotations.dataset)[i]\n",
    "    image = list(req_annotations.image)[i]\n",
    "    caption = f\"{text.lower().replace('.', '')}\"\n",
    "    image_path = images_dir + '/' + dataset + '/' + image\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "print(f\"Number of images: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1769a",
   "metadata": {},
   "source": [
    "# Prepare TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72e2bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [00:04<00:00, 1565.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 training examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:01<00:00, 1719.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 evaluation examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = 7500\n",
    "valid_size = 2500\n",
    "captions_per_image = 1\n",
    "images_per_file = 1\n",
    "\n",
    "train_image_paths = [path for path in image_paths if 'train' in path]\n",
    "num_train_files = int(np.ceil(train_size / images_per_file))\n",
    "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
    "\n",
    "valid_image_paths = [path for path in image_paths if 'valid' in path]\n",
    "num_valid_files = int(np.ceil(valid_size / images_per_file))\n",
    "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
    "\n",
    "tf.io.gfile.makedirs(tfrecords_dir)\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def create_example(image_path, caption):\n",
    "    feature = {\n",
    "        \"caption\": bytes_feature(caption.encode()),\n",
    "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_tfrecords(file_name, image_paths):\n",
    "    caption_list = []\n",
    "    image_path_list = []\n",
    "    for image_path in image_paths:\n",
    "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
    "        caption_list.extend(captions)\n",
    "        image_path_list.extend([image_path] * len(captions))\n",
    "\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for example_idx in range(len(image_path_list)):\n",
    "            example = create_example(\n",
    "                image_path_list[example_idx], caption_list[example_idx]\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    return example_idx + 1\n",
    "\n",
    "\n",
    "def write_data(image_paths, num_files, files_prefix):\n",
    "    example_counter = 0\n",
    "    for file_idx in tqdm(range(num_files)):\n",
    "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
    "        start_idx = images_per_file * file_idx\n",
    "        end_idx = start_idx + images_per_file\n",
    "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
    "    return example_counter\n",
    "\n",
    "\n",
    "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
    "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
    "\n",
    "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
    "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ffcb4c",
   "metadata": {},
   "source": [
    "# Implement Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1791f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    features = tf.io.parse_single_example(example, feature_description)\n",
    "    raw_image = features.pop(\"raw_image\")\n",
    "    features[\"image\"] = tf.image.resize(\n",
    "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, batch_size):\n",
    "\n",
    "    return (\n",
    "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
    "        .map(\n",
    "            read_example,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        .shuffle(batch_size * 10)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5333288",
   "metadata": {},
   "source": [
    "# Implement the projection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f98c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228120a",
   "metadata": {},
   "source": [
    "# Implement the vision encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1182484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vision_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained Xception model to be used as the base encoder.\n",
    "    xception = keras.applications.Xception(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = trainable\n",
    "    # Receive the images as inputs.\n",
    "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
    "    # Preprocess the input image.\n",
    "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    # Generate the embeddings for the images using the xception model.\n",
    "    embeddings = xception(xception_input)\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the vision encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"vision_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0564af",
   "metadata": {},
   "source": [
    "# Implement the text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8772ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the BERT preprocessing module.\n",
    "    preprocess = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n",
    "        name=\"text_preprocessing\",\n",
    "    )\n",
    "    # Load the pre-trained BERT model to be used as the base encoder.\n",
    "    bert = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "        \"bert\",\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    bert.trainable = trainable\n",
    "    # Receive the text as inputs.\n",
    "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    # Preprocess the text.\n",
    "    bert_inputs = preprocess(inputs)\n",
    "    # Generate embeddings for the preprocessed text using the BERT model.\n",
    "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the text encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54816bdf",
   "metadata": {},
   "source": [
    "# Implement the dual encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15fcaf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoder(keras.Model):\n",
    "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
    "        super(DualEncoder, self).__init__(**kwargs)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        # Place each encoder on a separate GPU (if available).\n",
    "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Get the embeddings for the captions.\n",
    "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
    "        with tf.device(\"/gpu:1\"):\n",
    "            # Get the embeddings for the images.\n",
    "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
    "        return caption_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = (\n",
    "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = tf.matmul(\n",
    "            image_embeddings, image_embeddings, transpose_b=True\n",
    "        )\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = tf.matmul(\n",
    "            caption_embeddings, caption_embeddings, transpose_b=True\n",
    "        )\n",
    "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = keras.activations.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
    "        )\n",
    "        # Compute the loss for the captions using crossentropy\n",
    "        captions_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        # Compute the loss for the images using crossentropy\n",
    "        images_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
    "        )\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            caption_embeddings, image_embeddings = self(features, training=True)\n",
    "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Monitor loss\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        caption_embeddings, image_embeddings = self(features, training=False)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49422b14",
   "metadata": {},
   "source": [
    "# Train the dual encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "810627e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5  # In practice, train for at least 30 epochs\n",
    "batch_size = 256\n",
    "\n",
    "vision_encoder = create_vision_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "text_encoder = create_text_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
    "dual_encoder.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94aaccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 0\n",
      "Number of examples (caption-image pairs): 7500\n",
      "Batch size: 256\n",
      "Steps per epoch: 30\n",
      "Epoch 1/5\n",
      "30/30 [==============================] - 2429s 81s/step - loss: 235.4948 - val_loss: 78.8963\n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 2410s 81s/step - loss: 90.9814 - val_loss: 44.8642\n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 2406s 81s/step - loss: 49.6884 - val_loss: 26.8933\n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 2411s 81s/step - loss: 32.0947 - val_loss: 16.1155\n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 2413s 81s/step - loss: 24.6816 - val_loss: 13.0429\n",
      "Training completed. Saving vision and text encoders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
    "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
    "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=3\n",
    ")\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "history = dual_encoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "print(\"Training completed. Saving vision and text encoders...\")\n",
    "vision_encoder.save(\"vision_encoder\")\n",
    "text_encoder.save(\"text_encoder\")\n",
    "print(\"Models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a914f4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91351619",
   "metadata": {},
   "source": [
    "# Generate Embeddings for the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd7954e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision and text encoders...\n",
      "Models are loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading vision and text encoders...\")\n",
    "vision_encoder = keras.models.load_model(\"vision_encoder\")\n",
    "text_encoder = keras.models.load_model(\"text_encoder\")\n",
    "print(\"Models are loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e197d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for /home/ec2-user/SageMaker/images/valid/1757.png ...\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "Image embeddings shape: (1, 256).\n"
     ]
    }
   ],
   "source": [
    "def read_image(image_path):\n",
    "    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "    return tf.image.resize(image_array, (299, 299))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91079e8",
   "metadata": {},
   "source": [
    "# Retrieve text for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1f92822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_reason(unique_texts, image_path, k=1, normalize=True):\n",
    "    # Get embedding for input image\n",
    "    get_image_embedding = vision_encoder.predict(\n",
    "        tf.expand_dims(read_image(image_path), axis=0),\n",
    "        verbose=0,\n",
    "    )\n",
    "    # Get text embeddings for all possible explanations\n",
    "    text_embeddings = [text_encoder(tf.convert_to_tensor([text.lower().replace('.', '')])) for text in unique_texts]\n",
    "    # Normalize text and image embedding\n",
    "    if normalize:\n",
    "        image_embedding = tf.math.l2_normalize(get_image_embedding)\n",
    "        text_embedding = [tf.math.l2_normalize(text) for text in text_embeddings]\n",
    "    # Obtain dot similarities\n",
    "    dot_similarity = [dot[0][0].numpy() for dot in tf.matmul(image_embedding, text_embedding, transpose_b=True)]\n",
    "    # Retrieve top index similarity score\n",
    "    result_index = dot_similarity.index(max(dot_similarity))\n",
    "    # Return best explanation sentence match\n",
    "    return unique_texts[result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ad6dce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_path_1 = '/home/ec2-user/SageMaker/images/valid/1757.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9a45c96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification for this image is asymmetric anaphase structure. This is explained by two chromosome clusters of unequal size are depicted.'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provide_reason(unique_texts, sample_image_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "24cf0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_path_2 = '/home/ec2-user/SageMaker/images/valid/16698.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b6fa7d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phase structure chromosome bridging most evidently described by chromosomes stretching from one pole to other.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provide_reason(unique_texts, sample_image_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8c80b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_path_3 = '/home/ec2-user/SageMaker/images/valid/18270.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b33738b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification for this image is asymmetric anaphase structure. This is explained by two chromosome clusters of unequal size are depicted.'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provide_reason(unique_texts, sample_image_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4fb03679",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_path_4 = '/home/ec2-user/SageMaker/images/valid/187098.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5d397cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All cells are healthy.'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provide_reason(unique_texts, sample_image_path_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9a1701c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_path_5 = '/home/ec2-user/SageMaker/images/valid/16743.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "57d07fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phase structure chromosome bridging most evidently described by chromosomes stretching from one pole to other.'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provide_reason(unique_texts, sample_image_path_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
